{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, warnings, json, time, ast, itertools\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import joblib\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as w\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "import networkx as nx\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import rankdata, pointbiserialr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupShuffleSplit, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import xgboost as xgb\n",
    "from collections import defaultdict\n",
    "\n",
    "import sklearn, warnings\n",
    "from packaging import version\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from boruta_shap import BorutaShap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CPU = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a816e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"datasets/train.csv\")\n",
    "test_raw  = pd.read_csv(\"datasets/test.csv\")\n",
    "assert train_raw.shape[0] == 10000, \"Expecting 10k training sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENT_CORE = {\n",
    "    \"pagerank\"    : nx.pagerank,\n",
    "    \"eigenvector\" : lambda G: nx.eigenvector_centrality_numpy(G),\n",
    "    \"voterank\"    : lambda G: {n: r/len(G) for r, n in\n",
    "                               enumerate(nx.voterank(G)[::-1], 1)},\n",
    "}\n",
    "\n",
    "CENT_EXTRA = {\n",
    "    \"degree\"      : nx.degree_centrality,\n",
    "    \"closeness\"   : nx.closeness_centrality,\n",
    "    \"harmonic\"    : nx.harmonic_centrality,\n",
    "    \"katz\"        : lambda G: nx.katz_centrality_numpy(G, alpha=0.01),\n",
    "    \"betweenness\": nx.betweenness_centrality,\n",
    "    \"load\"       : nx.load_centrality,\n",
    "}\n",
    "\n",
    "CENT_ALL = CENT_CORE | CENT_EXTRA\n",
    "LOW_BETTER = {\"eccentricity\", \"farness\", \"depth_leaf\"}#lower rank is better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nodes(df, keep_extra=True):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        G = nx.from_edgelist(ast.literal_eval(row.edgelist))\n",
    "        n_tok = len(G)\n",
    "\n",
    "        cents = {**{k: f(G) for k, f in CENT_CORE.items()},\n",
    "                 **({k: f(G) for k, f in CENT_EXTRA.items()} if keep_extra else {})}\n",
    "\n",
    "        #structural extras\n",
    "        ecc   = nx.eccentricity(G)\n",
    "        clos  = cents.get(\"closeness\", nx.closeness_centrality(G))\n",
    "        farness = {n: (1/ c if c else 0) for n, c in clos.items()}\n",
    "\n",
    "        leaves = [v for v in G if G.degree(v) == 1]\n",
    "        sp     = dict(nx.all_pairs_shortest_path_length(G))\n",
    "        depth_leaf = {v: max(sp[v][l] for l in leaves) for v in G}\n",
    "\n",
    "        #branchiness = leaves within 2 hops\n",
    "        branch2 = {}\n",
    "        for v in G:\n",
    "            branch2[v] = sum(1 for nbr,d in sp[v].items()\n",
    "                               if d <= 2 and G.degree(nbr) == 1 and nbr != v)\n",
    "\n",
    "        for n in G:\n",
    "            feat = {\n",
    "                \"language\" : row.language,\n",
    "                \"sentence\" : row.sentence,\n",
    "                \"node\"     : n,\n",
    "                \"n_tokens\" : n_tok,\n",
    "                **{k: v.get(n, 0.) for k, v in cents.items()},\n",
    "                \"eccentricity\": ecc[n],\n",
    "                \"farness\"     : farness[n],\n",
    "                \"depth_leaf\"  : depth_leaf[n],\n",
    "                \"branch2\"     : branch2[n],\n",
    "            }\n",
    "            if \"root\" in row:\n",
    "                feat[\"target\"] = int(n == row.root)\n",
    "            rows.append(feat)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    #scale raw centralities sentence-wise\n",
    "    raw_cols = list(CENT_CORE) + (list(CENT_EXTRA) if keep_extra else [])\n",
    "    df[raw_cols] = df.groupby(\"sentence\")[raw_cols] \\\n",
    "                     .transform(lambda x: MinMaxScaler().fit_transform(\n",
    "                                   x.values.reshape(-1,1)).ravel())\n",
    "\n",
    "    #position\n",
    "    df[\"pos_idx\"]  = df[\"node\"].astype(int) - 1\n",
    "    df[\"pos_frac\"] = df[\"pos_idx\"] / (df[\"n_tokens\"] - 1)\n",
    "    df[\"rev_idx\"]  = df[\"n_tokens\"] - df[\"pos_idx\"] - 1\n",
    "\n",
    "    #ranks + percentiles\n",
    "    rank_cols = raw_cols + [\"eccentricity\",\"farness\",\"branch_2\",\"depth_leaf\"]\n",
    "    for c in rank_cols:\n",
    "        rk = df.groupby(\"sentence\")[c] \\\n",
    "               .transform(lambda x: rankdata(\n",
    "                   x if c in LOW_BETTER else -x, method=\"average\"))\n",
    "        df[f\"{c}_rank\"] = rk\n",
    "        df[f\"{c}_pct\"]  = rk / (df[\"n_tokens\"] - 1)\n",
    "\n",
    "    core_pct = [f\"{c}_pct\" for c in [\"pagerank\",\"eigenvector\",\"voterank\"]]\n",
    "    df[\"meta_rank\"] = df[core_pct].mean(axis=1)\n",
    "    df[\"meta_rank_resid\"] = df[\"meta_rank\"] - \\\n",
    "                            df.groupby(\"language\")[\"meta_rank\"].transform(\"mean\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13aa964",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes = build_nodes(train_raw, keep_extra=True)\n",
    "test_nodes  = build_nodes(test_raw , keep_extra=True)\n",
    "\n",
    "BUCKET_EDGES  = [-1,5,10,15,20,25,30,40,1e9]\n",
    "BUCKET_LABELS = range(len(BUCKET_EDGES)-1)\n",
    "for df in (train_nodes, test_nodes):\n",
    "    df[\"bucket\"] = pd.cut(df[\"n_tokens\"], BUCKET_EDGES, labels=BUCKET_LABELS)\n",
    "    df[[\"language\",\"bucket\"]] = df[[\"language\",\"bucket\"]].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f664a7",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1560637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats\n",
    "summary = (\n",
    "    train_nodes\n",
    "      .drop_duplicates([\"language\", \"sentence\"])\n",
    "      .groupby(\"language\")[\"n_tokens\"]\n",
    "      .agg([\"count\", \"mean\", \"median\", \"min\", \"max\", \"std\"])\n",
    "      .sort_values(\"mean\", ascending=False)\n",
    ")\n",
    "print(summary.head(21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1620531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    test_nodes\n",
    "      .drop_duplicates([\"language\", \"sentence\"])\n",
    "      .groupby(\"language\")[\"n_tokens\"]\n",
    "      .agg([\"count\", \"mean\", \"median\", \"min\", \"max\", \"std\"])\n",
    "      .sort_values(\"mean\", ascending=False)\n",
    ")\n",
    "print(summary.head(21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c440ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length-based behaviour recognition:\n",
    "\n",
    "langs = sorted(train_nodes.language.unique())\n",
    "for LANG in langs:\n",
    "    roots = (\n",
    "        train_nodes.query(\"language == @LANG and target==1\")\n",
    "                   .drop_duplicates(\"sentence\")\n",
    "    )\n",
    "\n",
    "    x = roots[\"n_tokens\"].values\n",
    "    y = roots[\"pos_frac\"].values\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.scatter(x, y, s=10, alpha=0.4)\n",
    "\n",
    "    # LOWESS smoothing (fraction controls window)\n",
    "    lowess = sm.nonparametric.lowess(y, x, frac=0.2)\n",
    "    ax.plot(lowess[:,0], lowess[:,1], linewidth=2)\n",
    "\n",
    "    ax.set_xlabel(\"sentence length (tokens)\")\n",
    "    ax.set_ylabel(\"root relative position (0 = first, 1 = last)\")\n",
    "    ax.set_title(f\"{LANG}: root position vs length\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    bins = pd.cut(roots[\"n_tokens\"],\n",
    "              bins=[-1, 5, 10, 15, 20, 25, 30, 40, 1e9],\n",
    "              labels=[\"≤5\",\"5–10\",\"11–15\",\"16–20\",\"21–25\",\"26–30\",\"31–40\",\"41+\"])\n",
    "    bucket_stats = roots.groupby(bins)[\"pos_frac\"].agg([\"mean\", \"count\"])\n",
    "    print(bucket_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence-length distribution\n",
    "sent_train = (\n",
    "    train_nodes\n",
    "      .drop_duplicates(['language', 'sentence'])\n",
    "      .loc[:, ['language', 'n_tokens']]\n",
    ")\n",
    "\n",
    "sent_test  = (\n",
    "    test_nodes\n",
    "      .drop_duplicates(['language', 'sentence'])\n",
    "      .loc[:, ['language', 'n_tokens']]\n",
    ")\n",
    "\n",
    "def make_bins(data, mode='unit'):\n",
    "    if mode == 'unit':\n",
    "        lo, hi = data.min(), data.max()\n",
    "        return range(lo, hi + 2) \n",
    "    else:\n",
    "        edges = list(range(1, 71, 5))\n",
    "        return edges\n",
    "\n",
    "def plot_len_hist(language, dataset='train', binning='unit'):\n",
    "    df = sent_train if dataset == 'train' else sent_test\n",
    "    lengths = df.loc[df.language == language, 'n_tokens']\n",
    "    bins = make_bins(lengths, binning)\n",
    "\n",
    "    q05, q95 = lengths.quantile([0.05, 0.95]).values\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(lengths, bins=bins)\n",
    "\n",
    "    plt.axvline(q05, linestyle='--', label='q05')\n",
    "    plt.axvline(q95, linestyle='--', label='q95')\n",
    "\n",
    "    plt.xlabel('sentence length (tokens)')\n",
    "    plt.ylabel('number of sentences')\n",
    "    plt.title(f'Sentence-length distribution – {language} ({dataset})')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "langs = sorted(sent_train.language.unique())\n",
    "interact(\n",
    "    plot_len_hist,\n",
    "    language = langs,\n",
    "    dataset  = ['train', 'test'],\n",
    "    binning  = {'exact (bin = 1)': 'unit', 'bucketised': 'bucket'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#language vs root (absolute and relative)\n",
    "\n",
    "root_nodes = train_nodes.loc[train_nodes[\"target\"] == 1, [\"language\", \"sentence\", \"node\", \"n_tokens\"]].copy()\n",
    "\n",
    "root_nodes[\"rel\"] = root_nodes[\"node\"] / root_nodes[\"n_tokens\"]\n",
    "lang_order = train_nodes[\"language\"].drop_duplicates().tolist()\n",
    "\n",
    "def plot_sentence(sent_id: int, view: str = \"absolute\"):\n",
    "    df = (root_nodes[root_nodes[\"sentence\"] == sent_id]\n",
    "            .set_index(\"language\")\n",
    "            .loc[lang_order]\n",
    "            .reset_index())\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if view == \"absolute\":\n",
    "        #bars for sentence length\n",
    "        fig.add_bar(\n",
    "            x=df[\"language\"], y=df[\"n_tokens\"],\n",
    "            name=\"#tokens\",\n",
    "            marker_color=\"cornflowerblue\",\n",
    "            text=df[\"n_tokens\"], textposition=\"outside\"\n",
    "        )\n",
    "        #dots for root node id\n",
    "        fig.add_scatter(\n",
    "            x=df[\"language\"], y=df[\"node\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"root\",\n",
    "            marker=dict(size=11, color=\"crimson\")\n",
    "        )\n",
    "        y_title = \"value\"\n",
    "    else:  #relative\n",
    "        #dots at node/length\n",
    "        fig.add_scatter(\n",
    "            x=df[\"language\"], y=df[\"rel\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"root / length\",\n",
    "            marker=dict(size=11, color=\"crimson\")\n",
    "        )\n",
    "        fig.update_yaxes(range=[0, 1])\n",
    "        y_title = \"relative root position (0–1)\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Sentence {sent_id}: {'absolute' if view=='absolute' else 'relative'} view\",\n",
    "        yaxis_title=y_title,\n",
    "        xaxis=dict(categoryorder=\"array\", categoryarray=lang_order),\n",
    "        bargap=0.28 if view == \"absolute\" else 0.4,\n",
    "        width=980, height=480,\n",
    "        legend=dict(yanchor=\"top\", y=0.97, xanchor=\"left\", x=0.01)\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "sent_ids = sorted(root_nodes[\"sentence\"].unique())\n",
    "\n",
    "sent_slider = w.SelectionSlider(\n",
    "    options=sent_ids, value=sent_ids[0],\n",
    "    description=\"sentence:\", continuous_update=False\n",
    ")\n",
    "\n",
    "view_dd = w.Dropdown(\n",
    "    options=[(\"absolute\", \"absolute\"), (\"relative (0–1)\", \"relative\")],\n",
    "    value=\"absolute\",\n",
    "    description=\"view:\"\n",
    ")\n",
    "\n",
    "ui  = w.VBox([sent_slider, view_dd])\n",
    "out = w.interactive_output(\n",
    "        plot_sentence,\n",
    "        {\"sent_id\": sent_slider, \"view\": view_dd}\n",
    "      )\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"lang_len_prior\" not in train_nodes.columns:\n",
    "    def _add_prior(train_nodes, other_nodes):\n",
    "        roots = train_nodes.loc[train_nodes.target == 1,\n",
    "                                [\"language\", \"bucket\", \"pos_frac\"]]\n",
    "        tbl = roots.groupby([\"language\", \"bucket\"])[\"pos_frac\"].mean()\n",
    "\n",
    "        for df in (train_nodes, other_nodes):\n",
    "            idx = pd.MultiIndex.from_arrays([df[\"language\"], df[\"bucket\"]])\n",
    "            pr  = tbl.reindex(idx).to_numpy()\n",
    "            df[\"lang_len_prior\"]  = pr\n",
    "            df[\"dist_from_prior\"] = df[\"pos_frac\"] - pr\n",
    "\n",
    "    _add_prior(train_nodes, test_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d68944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#centralities\n",
    "EXTRA_PCT = [\"branch2_pct\", \"depth_leaf_pct\"]\n",
    "CENT_PCT   = [c + \"_pct\" for c in CENT_ALL] + [\"voterank_pct\"] + EXTRA_PCT\n",
    "CENT_PCT = [c+\"_pct\" for c in CENT_ALL] + [\"voterank_pct\"]\n",
    "rank_df = (\n",
    "    train_nodes\n",
    "      .melt(id_vars=[\"language\",\"sentence\",\"node\",\"target\"],\n",
    "            value_vars=CENT_PCT, var_name=\"cent\", value_name=\"val\")\n",
    "      .assign(rank=lambda d:\n",
    "              d.groupby([\"language\",\"sentence\",\"cent\"])[\"val\"]\n",
    "                .rank(\"min\", ascending=True))   # pct: smaller is better\n",
    "      .loc[lambda d: d.target==1]\n",
    ")\n",
    "\n",
    "disp = (rank_df.groupby([\"language\",\"cent\"])[\"rank\"]\n",
    "               .mean().unstack())\n",
    "disp.style.format(\"{:.1f}\").background_gradient(\"YlGn_r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cols = [c for c in train_nodes.columns\n",
    "#             if pd.api.types.is_numeric_dtype(train_nodes[c])\n",
    "#             and c not in [\"target\"]]\n",
    "# corr_tb = {c: pointbiserialr(train_nodes[\"target\"], train_nodes[c])[:2]\n",
    "#            for c in num_cols}\n",
    "# corr_df = pd.DataFrame(corr_tb, index=[\"r\",\"p\"]).T.sort_values(\"r\")\n",
    "# display(corr_df.tail(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['pos_frac','rev_idx','lang_len_prior','dist_from_prior'] + \\\n",
    "       [c for c in train_nodes.columns if c.endswith('_pct')]\n",
    "\n",
    "spearman = train_nodes[cols].corr(\"spearman\")\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.heatmap(spearman.abs(), cmap=\"viridis\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling NaNs\n",
    "print(\"NaNs in train:\", train_nodes[\"lang_len_prior\"].isna().sum())\n",
    "print(\"NaNs in  test:\", test_nodes [\"lang_len_prior\"].isna().sum())\n",
    "def missing_pairs(df):\n",
    "    return (\n",
    "        df.loc[df[\"lang_len_prior\"].isna(), [\"language\", \"bucket\"]]\n",
    "          .value_counts()\n",
    "          .rename(\"rows\")\n",
    "          .reset_index()\n",
    "          .sort_values(\"rows\", ascending=False)\n",
    "    )\n",
    "\n",
    "print(\"\\nMissing pairs in TRAIN:\")\n",
    "display(missing_pairs(train_nodes))\n",
    "\n",
    "print(\"\\nMissing pairs in TEST:\")\n",
    "display(missing_pairs(test_nodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_tbl = (train_nodes.loc[train_nodes.target == 1]\n",
    "                           .groupby([\"language\", \"bucket\"])[\"pos_frac\"]\n",
    "                           .mean())\n",
    "j0 = (test_nodes[\"language\"] == \"Japanese\") & (test_nodes[\"bucket\"] == 0)\n",
    "\n",
    "if j0.any():\n",
    "    jp_prior_b1 = prior_tbl[(\"Japanese\", 1)] #mean pos_frac for bucket 1\n",
    "    test_nodes.loc[j0, \"lang_len_prior\"]  = jp_prior_b1\n",
    "    test_nodes.loc[j0, \"dist_from_prior\"] = test_nodes.loc[j0, \"pos_frac\"] - jp_prior_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c86548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing pairs in TRAIN:\")\n",
    "display(missing_pairs(train_nodes))\n",
    "\n",
    "print(\"\\nMissing pairs in TEST:\")\n",
    "display(missing_pairs(test_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e2103",
   "metadata": {},
   "source": [
    "Feature selection with BorutaShap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [c for c in train_nodes.columns if c not in [\"language\",\"sentence\",\"node\",\"target\"]]\n",
    "\n",
    "X_full = train_nodes[FEATURES]\n",
    "y_full = train_nodes[\"target\"].values\n",
    "sid    = train_nodes[\"sentence\"].values\n",
    "\n",
    "boruta = BorutaShap(\n",
    "    model=LGBMClassifier(\n",
    "        n_estimators=400, learning_rate=0.03,\n",
    "        num_leaves=63, max_depth=-1, random_state=0\n",
    "    ),\n",
    "    importance_measure=\"shap\",\n",
    "    classification=True\n",
    ")\n",
    "N_ROWS = X_full.shape[0]\n",
    "boruta.fit(\n",
    "    X=X_full, y=y_full,\n",
    "    sample_weight=1.0 / train_nodes[\"n_tokens\"],\n",
    "    n_trials=100,\n",
    "    sample=False,\n",
    "    stratify=y_full,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "FEATURES = boruta.Subset().columns.tolist()\n",
    "print(\"Boruta kept\", len(FEATURES), \"features:\")\n",
    "print(FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec028e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dedublicated features from previous Boruta clean-up\n",
    "FEATURES = ['subgraph', 'pagerank_pct', 'branch2_pct',\n",
    "            'degree_pct', 'betweenness_pct', 'closeness',\n",
    "            'meta_rank_resid', 'eigenvector', 'farness',\n",
    "            'voterank_pct', 'harmonic_rank',\n",
    "            'lang_len_prior', 'degree',\n",
    "            'subgraph_rank', 'branch2_rank',\n",
    "            'dist_from_prior', 'pos_frac',\n",
    "            'language', 'bucket']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9127b20",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea491ed",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48652070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_language_sentence_accuracy(y, p, sid, lang):\n",
    "    df = pd.DataFrame({\"sid\": sid, \"lang\": lang, \"y\": y, \"p\": p})\n",
    "    idx_top = df.groupby([\"lang\", \"sid\"])[\"p\"].idxmax()\n",
    "    df_top  = df.loc[idx_top]\n",
    "\n",
    "    return df_top.groupby(\"lang\")[\"y\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logreg\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def sentence_level_table(y, p, sid, lang):\n",
    "    df = (pd.DataFrame({\"sid\": sid, \"language\": lang,\"y\": y,  \"p\": p})\n",
    "            .loc[lambda d: d.groupby(\"sid\")[\"p\"].idxmax()])\n",
    "    return df\n",
    "\n",
    "def update_lang_counters(df_sent, lang_ok, lang_tot):\n",
    "    correct = (df_sent[\"y\"] == 1).astype(int)\n",
    "    for g, cnt in correct.groupby(df_sent[\"language\"]):\n",
    "        lang_ok[g]  += cnt.sum()\n",
    "        lang_tot[g] += cnt.size\n",
    "\n",
    "CAT_COLS  = ['language', 'bucket']\n",
    "NUM_COLS  = [c for c in FEATURES if c not in CAT_COLS]\n",
    "\n",
    "X_full = train_nodes[FEATURES]\n",
    "y_full = train_nodes[\"target\"].values\n",
    "sid    = train_nodes[\"sentence\"].values\n",
    "\n",
    "ohe_kwargs = dict(handle_unknown=\"ignore\")\n",
    "if version.parse(sklearn.__version__) >= version.parse(\"1.2\"):\n",
    "    ohe_kwargs[\"sparse_output\"] = True\n",
    "else:\n",
    "    ohe_kwargs[\"sparse\"] = True\n",
    "\n",
    "ohe  = OneHotEncoder(**ohe_kwargs)\n",
    "prep = ColumnTransformer(\n",
    "        [(\"cat\", ohe, CAT_COLS),\n",
    "         (\"num\", \"passthrough\", NUM_COLS)],\n",
    "        remainder=\"drop\")\n",
    "\n",
    "logit = LogisticRegression(\n",
    "            penalty=\"l1\", solver=\"liblinear\",\n",
    "            C=0.8, max_iter=300, random_state=0)\n",
    "\n",
    "pipe = Pipeline([(\"prep\", prep), (\"clf\", logit)])\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "def acc_at1(y_true, y_pred, sent_ids):\n",
    "    top = (pd.DataFrame({\"sid\": sent_ids, \"y\": y_true, \"p\": y_pred})\n",
    "             .loc[lambda d: d.groupby(\"sid\")[\"p\"].idxmax(), \"y\"])\n",
    "    return accuracy_score(np.ones_like(top), top)\n",
    "\n",
    "#outer 5-fold SGKF\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "sent_ids = train_nodes.drop_duplicates(\"sentence\")[\"sentence\"].to_numpy()\n",
    "bucket_by_sid = (train_nodes\n",
    "                 .groupby(\"sentence\")[\"bucket\"]\n",
    "                 .agg(lambda s: s.value_counts().idxmax())\n",
    "                 .astype(str)\n",
    "                 .reindex(sent_ids))\n",
    "\n",
    "length_labels = bucket_by_sid.to_numpy()\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "acc_outer      = []\n",
    "node_true_all  = []\n",
    "node_pred_all  = []\n",
    "lang_ok        = defaultdict(int)\n",
    "lang_tot       = defaultdict(int)\n",
    "\n",
    "for tr_pos, va_pos in sgkf.split(np.zeros_like(sent_ids),\n",
    "                                 y=length_labels,\n",
    "                                 groups=sent_ids):\n",
    "\n",
    "    sent_tr = sent_ids[tr_pos]\n",
    "    sent_va = sent_ids[va_pos]\n",
    "\n",
    "    tr_mask = train_nodes[\"sentence\"].isin(sent_tr)\n",
    "    va_mask = train_nodes[\"sentence\"].isin(sent_va)\n",
    "\n",
    "    pipe.fit(X_full.loc[tr_mask], y_full[tr_mask],\n",
    "             clf__sample_weight=1.0 /\n",
    "                                train_nodes.loc[tr_mask, \"n_tokens\"])\n",
    "\n",
    "    #node-level predictions on current outer-val fold\n",
    "    y_pred_nodes = pipe.predict_proba(X_full.loc[va_mask])[:, 1]\n",
    "    y_true_nodes = y_full[va_mask]\n",
    "\n",
    "    node_pred_all.append(y_pred_nodes)\n",
    "    node_true_all.append(y_true_nodes)\n",
    "\n",
    "    #sentence-level accuracy\n",
    "    acc_outer.append(acc_at1(y_true_nodes, y_pred_nodes,sid[va_mask]))\n",
    "\n",
    "    #per-language counts\n",
    "    df_val_sent = (pd.DataFrame({\"sid\": sid[va_mask],\n",
    "                                 \"language\": train_nodes.loc[va_mask,\"language\"],\n",
    "                                 \"y\": y_true_nodes,\n",
    "                                 \"p\": y_pred_nodes})\n",
    "                     .loc[lambda d: d.groupby(\"sid\")[\"p\"].idxmax()])\n",
    "\n",
    "    for g, cnt in (df_val_sent[\"y\"] == 1).groupby(df_val_sent[\"language\"]):\n",
    "        lang_ok[g]  += cnt.sum()\n",
    "        lang_tot[g] += cnt.size\n",
    "\n",
    "\n",
    "print(f\"LogReg outer-CV accuracy@1 : \"\n",
    "      f\"{np.mean(acc_outer):.3f} ± {np.std(acc_outer):.3f}\")\n",
    "\n",
    "# per-language sentence accuracy\n",
    "print(\"\\nPer-language sentence accuracy:\")\n",
    "for g, a in sorted({g: lang_ok[g]/lang_tot[g] for g in lang_ok}.items(),\n",
    "                   key=lambda x: -x[1]):\n",
    "    print(f\"  {g:>3s} : {a:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lang_fold = per_language_sentence_accuracy(y_true_nodes, y_pred_nodes, sid[va_mask],train_nodes.loc[va_mask, \"language\"])\n",
    "for g, a in acc_lang_fold.items():\n",
    "    lang_ok[g]  += a * acc_lang_fold.size \n",
    "    lang_tot[g] += acc_lang_fold.size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af40582",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPer-language sentence accuracy:\")\n",
    "for g, a in sorted({g: lang_ok[g]/lang_tot[g] for g in lang_tot}.items(),\n",
    "                   key=lambda x: -x[1]):\n",
    "    print(f\"  {g:>10s} : {a:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_by_lang_sentence(y, p, sid, lang):\n",
    "    \"\"\"Return df with one row per (language, sentence).\"\"\"\n",
    "    df_nodes = pd.DataFrame({\"sid\": sid, \"lang\": lang, \"y\": y, \"p\": p})\n",
    "    idx_best = df_nodes.groupby([\"lang\", \"sid\"])[\"p\"].idxmax()\n",
    "    return df_nodes.loc[idx_best]\n",
    "\n",
    "def update_lang_totals(df_sent, correct_tot, all_tot):\n",
    "    \"\"\"Increment per-language correct / total counters.\"\"\"\n",
    "    for g, grp in df_sent.groupby(\"lang\"):\n",
    "        all_tot[g]     += len(grp)\n",
    "        correct_tot[g] += grp[\"y\"].sum()\n",
    "\n",
    "acc_global_cv  = []\n",
    "lang_correct   = defaultdict(int)\n",
    "lang_total     = defaultdict(int)\n",
    "node_pred_all  = []\n",
    "node_true_all  = []\n",
    "\n",
    "for tr_pos, va_pos in sgkf.split(np.zeros_like(sent_ids), y=length_labels, groups=sent_ids):\n",
    "\n",
    "    sent_tr = sent_ids[tr_pos]\n",
    "    sent_va = sent_ids[va_pos]\n",
    "\n",
    "    tr_mask = train_nodes[\"sentence\"].isin(sent_tr)\n",
    "    va_mask = train_nodes[\"sentence\"].isin(sent_va)\n",
    "\n",
    "    pipe.fit(X_full.loc[tr_mask], y_full[tr_mask],\n",
    "             clf__sample_weight=1.0 /\n",
    "                                train_nodes.loc[tr_mask, \"n_tokens\"])\n",
    "\n",
    "    y_pred_nodes = pipe.predict_proba(X_full.loc[va_mask])[:, 1]\n",
    "    y_true_nodes = y_full[va_mask]\n",
    "    node_pred_all.append(y_pred_nodes)\n",
    "    node_true_all.append(y_true_nodes)\n",
    "\n",
    "    df_sent = collapse_by_lang_sentence(y_true_nodes, y_pred_nodes, sid[va_mask], train_nodes.loc[va_mask,\"language\"])\n",
    "\n",
    "    acc_global_cv.append(df_sent[\"y\"].mean())\n",
    "    update_lang_totals(df_sent, lang_correct, lang_total)\n",
    "\n",
    "print(f\"LogReg outer-CV accuracy@1 : \"\n",
    "      f\"{np.mean(acc_global_cv):.3f} ± {np.std(acc_global_cv):.3f}\")\n",
    "\n",
    "print(\"\\nPer-language sentence accuracy:\")\n",
    "for g, acc in sorted({g: lang_correct[g]/lang_total[g]\n",
    "                      for g in lang_total}.items(),\n",
    "                     key=lambda x: -x[1]):\n",
    "    print(f\"  {g:>10s} : {acc:.3f}   (n={lang_total[g]})\")\n",
    "\n",
    "# optional node-level metrics\n",
    "# y_nodes = np.concatenate(node_true_all)\n",
    "# p_nodes = np.concatenate(node_pred_all)\n",
    "# print(f\"\\nNode-level ROC-AUC          : {roc_auc_score(y_nodes, p_nodes):.3f}\")\n",
    "# print(f\"Node-level average precision : \"\n",
    "#       f\"{average_precision_score(y_nodes, p_nodes):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3683c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 boosting models + Random Forest\n",
    "# outer 5-fold StratifiedGroupKFold on (sentence, bucket)\n",
    "# inner 90 / 10 GroupShuffleSplit + Optuna tuning (no group leakage)\n",
    "# models: LightGBM, CatBoost, XGBoost, HistGradientBoosting, Random Forest\n",
    "# accuracy@1 on (language, sentence) + per-language report\n",
    "# deterministic seeds, Optuna SuccessiveHalving pruner\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "PRUNER = optuna.pruners.SuccessiveHalvingPruner(min_resource=10)\n",
    "\n",
    "FEATURES = [\"subgraph\",\"pagerank_pct\",\"branch2_pct\",\"degree_pct\",\n",
    "            \"betweenness_pct\",\"closeness\",\"meta_rank_resid\",\"eigenvector\",\n",
    "            \"farness\",\"voterank_pct\",\"harmonic_rank\",\"lang_len_prior\",\n",
    "            \"degree\",\"subgraph_rank\",\"branch2_rank\",\"dist_from_prior\",\n",
    "            \"pos_frac\",\"n_tokens\",\"language\",\"bucket\"]\n",
    "CAT_COLS  = [\"language\",\"bucket\"]\n",
    "\n",
    "X_full    = train_nodes[FEATURES]\n",
    "y_full    = train_nodes[\"target\"].to_numpy()\n",
    "sid_full  = train_nodes[\"sentence\"].astype(\"int64\").to_numpy()\n",
    "lang_full = train_nodes[\"language\"].to_numpy()\n",
    "w_full    = 1.0 / np.sqrt(train_nodes[\"n_tokens\"].to_numpy())\n",
    "CAT_IDX   = [X_full.columns.get_loc(c) for c in CAT_COLS]\n",
    "\n",
    "# numeric frame for models that need it\n",
    "cat_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "X_enc = X_full.copy()\n",
    "X_enc[CAT_COLS] = cat_enc.fit_transform(X_full[CAT_COLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0748e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "class _ConsoleLogger:\n",
    "    def __init__(self):\n",
    "        self.best = None\n",
    "        self.best_id = None\n",
    "    def __call__(self, study, trial):\n",
    "        self.best     = study.best_value\n",
    "        self.best_id  = study.best_trial.number\n",
    "        msg = (f\"[I] {time.strftime('%H:%M:%S')}  Trial {trial.number} \"\n",
    "               f\"finished value: {trial.value:.6f}.  \"\n",
    "               f\"Best is {self.best_id} ({self.best:.6f})\")\n",
    "        print(msg, file=sys.stderr)\n",
    "\n",
    "\n",
    "def timeit_step(tag):\n",
    "    def _wrap(fn):\n",
    "        def inner(*a, **k):\n",
    "            t0 = time.time()\n",
    "            res = fn(*a, **k)\n",
    "            tqdm.write(f\"      ↳ {tag} took {time.time()-t0:5.1f}s\")\n",
    "            return res\n",
    "        return inner\n",
    "    return _wrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9144c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_group_val_split(X, y, groups, val=0.10, seed=SEED):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val, random_state=seed)\n",
    "    return next(gss.split(X, y, groups))\n",
    "\n",
    "def acc_at1(y, p, sid, lang):\n",
    "    df = pd.DataFrame({\"sid\": sid, \"lang\": lang, \"y\": y, \"p\": p})\n",
    "    top = df.loc[df.groupby([\"lang\", \"sid\"])[\"p\"].idxmax(), \"y\"]\n",
    "    return (top == 1).mean()\n",
    "\n",
    "def lang_acc_series(y, p, sid, lang):\n",
    "    df = pd.DataFrame({\"sid\": sid, \"lang\": lang, \"y\": y, \"p\": p})\n",
    "    idx = df.groupby([\"lang\", \"sid\"])[\"p\"].idxmax()\n",
    "    return df.loc[idx].groupby(\"lang\")[\"y\"].mean()\n",
    "\n",
    "def _split_inner(X, y, sid, lang, w):\n",
    "    tr, va = make_group_val_split(X, y, sid)\n",
    "    return X.iloc[tr], X.iloc[va], y[tr], y[va], sid[tr], sid[va], lang[tr], lang[va], w[tr], w[va]\n",
    "\n",
    "def _group_lengths_lang_sid(lang, sid):\n",
    "    \"\"\"Group lengths for each (lang, sid) in the original row order.\"\"\"\n",
    "    key = pd.Series(list(zip(lang, sid)))\n",
    "    return (key.value_counts(sort=False).loc[key.unique()].to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0243f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcs for optuna\n",
    "def obj_lgb(trial, X, y, sid, lang, w):\n",
    "    Xt, Xv, yt, yv, sidt, sidv, langt, langv, wt, wv = _split_inner(X, y, sid, lang, w)\n",
    "    params = dict(objective=\"binary\", metric=\"binary_logloss\", random_state=SEED,\n",
    "                  learning_rate=trial.suggest_float(\"learning_rate\",0.01,0.2,log=True),\n",
    "                  num_leaves=trial.suggest_int(\"num_leaves\",31,255),\n",
    "                  feature_fraction=trial.suggest_float(\"feature_fraction\",0.6,1.0),\n",
    "                  bagging_fraction=trial.suggest_float(\"bagging_fraction\",0.6,1.0),\n",
    "                  bagging_freq=5,\n",
    "                  num_threads=N_CPU,\n",
    "                  lambda_l1=trial.suggest_float(\"lambda_l1\",0.0,2.0),\n",
    "                  lambda_l2=trial.suggest_float(\"lambda_l2\",0.0,2.0),\n",
    "                  verbose=-1, force_col_wise=True)\n",
    "    mdl = lgb.LGBMClassifier(**params, n_estimators=3000)\n",
    "    mdl.fit(Xt, yt, sample_weight=wt, categorical_feature=CAT_IDX,\n",
    "            eval_set=[(Xv, yv)], eval_sample_weight=[wv],\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    trial.set_user_attr(\"n_round\", mdl.best_iteration_ or 3000)\n",
    "    p = mdl.predict_proba(Xv, num_iteration=mdl.best_iteration_)[:,1]\n",
    "    return 1 - acc_at1(yv, p, sidv, langv)\n",
    "\n",
    "\n",
    "def obj_lgb_rank(trial, X, y, sid, lang, w):\n",
    "    Xt, Xv, yt, yv, sidt, sidv, langt, langv, wt, wv = \\\n",
    "        _split_inner(X, y, sid, lang, w)\n",
    "\n",
    "    g_tr = _group_lengths_lang_sid(langt, sidt)\n",
    "    g_va = _group_lengths_lang_sid(langv, sidv)\n",
    "\n",
    "    params = dict(\n",
    "        objective        = \"lambdarank\",\n",
    "        metric           = \"ndcg\",\n",
    "        eval_at          = [1],\n",
    "        random_state     = SEED,\n",
    "        learning_rate    = trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        num_leaves       = trial.suggest_int(\"num_leaves\", 31, 255),\n",
    "        feature_fraction = trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        bagging_fraction = trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "        bagging_freq     = 5,\n",
    "        lambda_l1        = trial.suggest_float(\"lambda_l1\", 0.0, 2.0),\n",
    "        lambda_l2        = trial.suggest_float(\"lambda_l2\", 0.0, 2.0),\n",
    "        verbose          = -1,\n",
    "        num_threads      = N_CPU,\n",
    "    )\n",
    "\n",
    "    rk = lgb.LGBMRanker(**params, n_estimators=3000)\n",
    "    rk.fit(Xt, yt,\n",
    "           group=g_tr,\n",
    "           eval_set=[(Xv, yv)],\n",
    "           eval_group=[g_va],\n",
    "           sample_weight=wt,\n",
    "           eval_sample_weight=[wv],\n",
    "           callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "\n",
    "    trial.set_user_attr(\"n_round\", rk.best_iteration_ or 3000)\n",
    "\n",
    "    p_val = rk.predict(Xv, num_iteration=rk.best_iteration_)\n",
    "    return 1 - acc_at1(yv, p_val, sidv, langv)\n",
    "\n",
    "\n",
    "def obj_cat(trial, X, y, sid, lang, w):\n",
    "    Xt, Xv, yt, yv, sidt, sidv, langt, langv, wt, wv = _split_inner(X, y, sid, lang, w)\n",
    "    train_pool = Pool(data=Xt, label=yt, cat_features=CAT_IDX, weight=wt)\n",
    "    valid_pool = Pool(data=Xv, label=yv, cat_features=CAT_IDX, weight=wv)\n",
    "\n",
    "    params = dict(\n",
    "        loss_function=\"Logloss\",\n",
    "        random_seed=SEED,\n",
    "        verbose=False,\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        depth=trial.suggest_int(\"depth\", 4, 10),\n",
    "        l2_leaf_reg=trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "        border_count=trial.suggest_int(\"border_count\", 32, 255),\n",
    "        thread_count=N_CPU,\n",
    "    )\n",
    "\n",
    "    mdl = CatBoostClassifier(**params, iterations=3000)\n",
    "    mdl.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=50)\n",
    "\n",
    "    best_iter = mdl.get_best_iteration() or 3000\n",
    "    trial.set_user_attr(\"n_round\", best_iter)\n",
    "\n",
    "    p = mdl.predict_proba(valid_pool)[:, 1]\n",
    "    return 1 - acc_at1(yv, p, sidv, langv)\n",
    "\n",
    "\n",
    "def obj_xgb(trial, X, y, sid, lang, w):\n",
    "    import xgboost as xgb\n",
    "    Xt, Xv, yt, yv, sidt, sidv, langt, langv, wt, wv = _split_inner(X, y, sid, lang, w)\n",
    "    Xt = Xt.copy();  Xv = Xv.copy()\n",
    "    for c in CAT_COLS:\n",
    "        Xt[c] = Xt[c].cat.codes\n",
    "        Xv[c] = Xv[c].cat.codes\n",
    "    dtr = xgb.DMatrix(Xt, label=yt, weight=wt)\n",
    "    dva = xgb.DMatrix(Xv, label=yv, weight=wv)\n",
    "    params = dict(objective=\"binary:logistic\", seed=SEED,\n",
    "                  learning_rate=trial.suggest_float(\"learning_rate\",0.01,0.2,log=True),\n",
    "                  max_depth=trial.suggest_int(\"max_depth\",3,10),\n",
    "                  min_child_weight=trial.suggest_int(\"min_child_weight\",1,10),\n",
    "                  subsample=trial.suggest_float(\"subsample\",0.6,1.0),\n",
    "                  colsample_bytree=trial.suggest_float(\"colsample_bytree\",0.6,1.0),\n",
    "                  reg_lambda=trial.suggest_float(\"reg_lambda\",0.0,5.0),\n",
    "                  alpha=trial.suggest_float(\"alpha\",0.0,5.0),\n",
    "                  nthread=N_CPU,\n",
    "                  tree_method=\"hist\", eval_metric=\"logloss\")\n",
    "    booster = xgb.train(params, dtr, 4000,\n",
    "                        evals=[(dva,\"val\")], early_stopping_rounds=50,\n",
    "                        verbose_eval=False)\n",
    "    trial.set_user_attr(\"n_round\", booster.best_iteration or 3000)\n",
    "    p = booster.predict(dva, iteration_range=(0, booster.best_iteration))\n",
    "    return 1 - acc_at1(yv, p, sidv, langv)\n",
    "\n",
    "\n",
    "def obj_hgb(trial, X, y, sid, lang, w):\n",
    "    X_num = X_enc.loc[X.index] \n",
    "    Xt, Xv, yt, yv, sidt, sidv, langt, langv, wt, wv = \\\n",
    "        _split_inner(X_num, y, sid, lang, w)\n",
    "\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        learning_rate    = trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        max_depth        = trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        max_leaf_nodes   = trial.suggest_int(\"max_leaf_nodes\", 31, 255),\n",
    "        l2_regularization= trial.suggest_float(\"l2_regularization\", 0.0, 2.0),\n",
    "        min_samples_leaf = 20,\n",
    "        random_state     = SEED,\n",
    "    )\n",
    "    hgb.fit(Xt, yt, sample_weight=wt)\n",
    "    p = hgb.predict_proba(Xv)[:, 1]\n",
    "    return 1 - acc_at1(yv, p, sidv, langv)\n",
    "\n",
    "\n",
    "\n",
    "def obj_rf(trial, X, y, sid, lang, w):\n",
    "    X_num = X_enc.loc[X.index]\n",
    "    Xt, Xv, yt, yv, sidt, sidv, langt, langv, wt, wv = _split_inner(X_num, y, sid, lang, w)\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators       = trial.suggest_int(\"n_estimators\", 200, 800, 100),\n",
    "        max_depth          = trial.suggest_int(\"max_depth\", 5, 20),\n",
    "        max_features       = trial.suggest_float(\"max_features\", 0.3, 1.0),\n",
    "        min_samples_leaf   = trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        bootstrap          = True,\n",
    "        n_jobs             = -1,\n",
    "        random_state       = SEED,\n",
    "    )\n",
    "    rf.fit(Xt, yt, sample_weight=wt)\n",
    "    p = rf.predict_proba(Xv)[:, 1]\n",
    "    return 1 - acc_at1(yv, p, sidv, langv)\n",
    "\n",
    "\n",
    "OBJECTIVES = {\"lightgbm\": obj_lgb,\n",
    "              \"lgbm_ranker\" : obj_lgb_rank,\n",
    "              \"catboost\": obj_cat,\n",
    "              \"xgboost\":  obj_xgb,\n",
    "              \"hist_gbm\": obj_hgb,\n",
    "              \"random_forest\": obj_rf}\n",
    "\n",
    "#optuna\n",
    "class _TqdmOptuna:\n",
    "    def __init__(self, n_trials):\n",
    "        self.pb = tqdm(total=n_trials, desc=\"Optuna\", leave=False, unit=\"trial\")\n",
    "    def __call__(self, study, trial):\n",
    "        best = 1 - study.best_value if study.best_value is not None else None\n",
    "        if best is not None:\n",
    "            self.pb.set_postfix(best=f\"{best:.3f}\")\n",
    "        self.pb.update(1)\n",
    "    def close(self):\n",
    "        self.pb.close()\n",
    "\n",
    "def tune_model(key, X, y, sid, lang, w, n_trials=30):\n",
    "    bar    = _TqdmOptuna(n_trials)\n",
    "    logger = _ConsoleLogger()\n",
    "    study  = optuna.create_study(direction=\"minimize\",\n",
    "                                 pruner=PRUNER,\n",
    "                                 sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "    study.optimize(lambda t: OBJECTIVES[key](t, X, y, sid, lang, w),\n",
    "                   n_trials=n_trials,\n",
    "                   n_jobs=8,\n",
    "                   callbacks=[bar, logger],\n",
    "                   show_progress_bar=False)\n",
    "    bar.close()\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outer Cross-Validation\n",
    "sent_df = train_nodes.drop_duplicates(\"sentence\")\n",
    "sent_ids      = sent_df[\"sentence\"].to_numpy()\n",
    "bucket_labels = (train_nodes.groupby(\"sentence\")[\"bucket\"]\n",
    "                 .agg(lambda s: s.value_counts().idxmax())\n",
    "                 .astype(str)\n",
    "                 .reindex(sent_ids).to_numpy())\n",
    "SGKF = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "results, best_folds = {}, {}\n",
    "\n",
    "\n",
    "def run_model(model_key, X_full=X_full, X_enc=X_enc, w_full=w_full):\n",
    "    fold_scores, per_fold_params = [], []\n",
    "    lang_ok, lang_tot = defaultdict(int), defaultdict(int)\n",
    "    best_so_far = 0.0\n",
    "\n",
    "    out_dir = Path(\"models\") / model_key\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for fold, (tr_pos, va_pos) in enumerate(\n",
    "            SGKF.split(np.zeros_like(sent_ids),\n",
    "                       y=bucket_labels, groups=sent_ids), 1):\n",
    "\n",
    "        tqdm.write(f\"- {model_key}  fold {fold}/5\")\n",
    "\n",
    "        tr_mask = train_nodes[\"sentence\"].isin(sent_ids[tr_pos])\n",
    "        va_mask = train_nodes[\"sentence\"].isin(sent_ids[va_pos])\n",
    "\n",
    "        #tune on outer-train\n",
    "        study = timeit_step(\"Optuna\")(tune_model)(\n",
    "                           model_key,\n",
    "                           X_full.loc[tr_mask], y_full[tr_mask],\n",
    "                           sid_full[tr_mask],   lang_full[tr_mask],\n",
    "                           w_full[tr_mask])\n",
    "\n",
    "        bp = study.best_params\n",
    "        br = study.user_attrs.get(\"n_round\", 3000)\n",
    "        per_fold_params.append((bp, br))\n",
    "\n",
    "        #refit and predict\n",
    "        @timeit_step(\"refit+predict\")\n",
    "        def _fit_predict():\n",
    "            if model_key == \"lightgbm\":\n",
    "                import lightgbm as lgb\n",
    "                mdl = lgb.LGBMClassifier(**bp, n_estimators=br, random_state=SEED)\n",
    "                mdl.fit(X_full.loc[tr_mask], y_full[tr_mask],\n",
    "                        sample_weight=w_full[tr_mask],\n",
    "                        categorical_feature=CAT_IDX)\n",
    "                return mdl.predict_proba(X_full.loc[va_mask])[:,1], mdl\n",
    "\n",
    "            elif model_key == \"catboost\":\n",
    "                from catboost import CatBoostClassifier, Pool\n",
    "                train_pool = Pool(\n",
    "                    X_full.loc[tr_mask], y_full[tr_mask],\n",
    "                    cat_features = CAT_IDX,\n",
    "                    weight       = w_full[tr_mask]\n",
    "                )\n",
    "                valid_pool = Pool(\n",
    "                    X_full.loc[va_mask], label=None,\n",
    "                    cat_features = CAT_IDX\n",
    "                )\n",
    "                mdl = CatBoostClassifier(**bp, iterations=br, random_seed=SEED, verbose=False)\n",
    "                mdl.fit(train_pool)\n",
    "                return mdl.predict_proba(valid_pool)[:,1], mdl\n",
    "\n",
    "\n",
    "            elif model_key == \"xgboost\":\n",
    "                import xgboost as xgb\n",
    "                X_tr = X_full.loc[tr_mask].copy();  X_va = X_full.loc[va_mask].copy()\n",
    "                for c in CAT_COLS:\n",
    "                    X_tr[c] = X_tr[c].cat.codes;  X_va[c] = X_va[c].cat.codes\n",
    "                booster = xgb.train(bp,\n",
    "                                    xgb.DMatrix(X_tr,label=y_full[tr_mask],weight=w_full[tr_mask]),\n",
    "                                    num_boost_round=br, verbose_eval=False)\n",
    "                return booster.predict(xgb.DMatrix(X_va)), booster\n",
    "\n",
    "            elif model_key == \"hist_gbm\":\n",
    "                mdl = HistGradientBoostingClassifier(**bp, random_state=SEED)\n",
    "                mdl.fit(X_enc.loc[tr_mask], y_full[tr_mask],\n",
    "                        sample_weight=w_full[tr_mask])\n",
    "                return mdl.predict_proba(X_enc.loc[va_mask])[:,1], mdl\n",
    "\n",
    "            elif model_key == \"lgbm_ranker\":\n",
    "                import lightgbm as lgb\n",
    "                g_tr = _group_lengths(sid_full[tr_mask])\n",
    "                rk = lgb.LGBMRanker(**bp, n_estimators=br, random_state=SEED)\n",
    "                rk.fit(X_full.loc[tr_mask], y_full[tr_mask],\n",
    "                    group=g_tr,\n",
    "                    sample_weight=w_full[tr_mask],\n",
    "                    categorical_feature=CAT_IDX)\n",
    "                return rk.predict(X_full.loc[va_mask], num_iteration=br), rk\n",
    "\n",
    "\n",
    "            else:  # random_forest\n",
    "                mdl = RandomForestClassifier(**bp, n_jobs=-1, random_state=SEED)\n",
    "                mdl.fit(X_enc.loc[tr_mask], y_full[tr_mask],\n",
    "                        sample_weight=w_full[tr_mask])\n",
    "                return mdl.predict_proba(X_enc.loc[va_mask])[:,1], mdl\n",
    "\n",
    "        p_val, mdl = _fit_predict()\n",
    "\n",
    "        # save model + params\n",
    "        joblib.dump(mdl, out_dir / f\"fold{fold}.pkl\")\n",
    "        with open(out_dir / f\"fold{fold}_params.json\",\"w\") as f:\n",
    "            json.dump({\"params\":bp,\"best_round\":br}, f)\n",
    "\n",
    "        # fold metrics\n",
    "        acc = acc_at1(y_full[va_mask], p_val, sid_full[va_mask], lang_full[va_mask])\n",
    "        fold_scores.append(acc);  best_so_far = max(best_so_far, acc)\n",
    "\n",
    "        lg = lang_acc_series(y_full[va_mask], p_val, sid_full[va_mask], lang_full[va_mask])\n",
    "        for g,a in lg.items():\n",
    "            lang_ok[g]  += a * lg.count()\n",
    "            lang_tot[g] += lg.count()\n",
    "\n",
    "        tqdm.write(f\"    acc@1={acc:.3f}  best_so_far={best_so_far:.3f} | \"\n",
    "                   f\"{', '.join(f'{g}:{lg[g]:.3f}' for g in lg.head(21).index)}\")\n",
    "\n",
    "    # summary\n",
    "    mean_acc, std_acc = float(np.mean(fold_scores)), float(np.std(fold_scores))\n",
    "    summary = {\"mean_acc\": mean_acc,\n",
    "               \"std_acc\": std_acc,\n",
    "               \"fold_acc\": fold_scores,\n",
    "               \"fold_params\": [dict(p) for p,_ in per_fold_params]}\n",
    "    with open(out_dir / \"cv_summary.json\",\"w\") as f:\n",
    "        json.dump(summary,f,indent=2)\n",
    "\n",
    "    print(f\"\\n{model_key}  acc@1 = {mean_acc:.3f} ± {std_acc:.3f}\")\n",
    "    for g,a in sorted({g: lang_ok[g]/lang_tot[g] for g in lang_tot}.items(),\n",
    "                      key=lambda x:-x[1]):\n",
    "        print(f\"  {g:>10s}: {a:.3f}\")\n",
    "    print(\"-\"*46)\n",
    "    return mean_acc,std_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank models by outer-CV mean 0-1 loss\n",
    "\n",
    "K_TOP = 6\n",
    "ranked = sorted(results.items(), key=lambda kv: -kv[1][0])[:K_TOP]\n",
    "print(\"\\n⸻ Model leader-board (outer-CV) ⸻\")\n",
    "for i,(k,(m,_)) in enumerate(ranked,1):\n",
    "    print(f\"{i:>2}. {k:15s}  {m:.3f}\")\n",
    "\n",
    "#train each top model on ALL data and save predictions\n",
    "prob_cols   = []\n",
    "weights_w   = []\n",
    "TEST_PROB_DIR = Path(\"test_probs\"); TEST_PROB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def _fold_param_aggregate(key):\n",
    "    fold_dir = Path(\"models\")/key\n",
    "    js  = sorted(fold_dir.glob(\"fold*_params.json\"))\n",
    "    spec= [json.load(j.open()) for j in js]\n",
    "    fold_scores = np.loadtxt(fold_dir/\"cv_summary.json\", ndmin=1, dtype=float, usecols=0, max_rows=len(js))\n",
    "    best_idx = int(np.argmin(fold_scores)) if len(js)==len(fold_scores) else 0\n",
    "    p_best   = spec[best_idx][\"params\"]\n",
    "    n_best   = spec[best_idx][\"best_round\"]\n",
    "    return p_best, n_best or 3000\n",
    "\n",
    "for key,_ in ranked:\n",
    "    print(f\"\\n- training full-corpus {key}\")\n",
    "    best_params, n_round = _fold_param_aggregate(key)\n",
    "\n",
    "    if key==\"lightgbm\":\n",
    "        import lightgbm as lgb\n",
    "        mdl = lgb.LGBMClassifier(**best_params, n_estimators=n_round,\n",
    "                                 random_state=SEED)\n",
    "        mdl.fit(X_full, y_full, sample_weight=w_full,\n",
    "                categorical_feature=CAT_IDX)\n",
    "\n",
    "    elif key==\"lgbm_ranker\":\n",
    "        import lightgbm as lgb\n",
    "        g_all = (pd.Series(sid_full)\n",
    "                   .value_counts(sort=False)\n",
    "                   .loc[pd.Series(sid_full).unique()]\n",
    "                   .to_numpy())\n",
    "        mdl = lgb.LGBMRanker(**best_params, n_estimators=n_round,\n",
    "                             random_state=SEED)\n",
    "        mdl.fit(X_full, y_full, group=g_all,\n",
    "                sample_weight=w_full, categorical_feature=CAT_IDX)\n",
    "\n",
    "    elif key==\"catboost\":\n",
    "        from catboost import CatBoostClassifier, Pool\n",
    "        pool = Pool(X_full, y_full, cat_features=CAT_IDX, weight=w_full)\n",
    "        mdl  = CatBoostClassifier(**best_params, iterations=n_round,\n",
    "                                  random_seed=SEED, verbose=False)\n",
    "        mdl.fit(pool)\n",
    "\n",
    "    elif key==\"xgboost\":\n",
    "        import xgboost as xgb\n",
    "        X_num = X_full.copy()\n",
    "        for c in CAT_COLS: X_num[c] = X_num[c].cat.codes\n",
    "        dtrain = xgb.DMatrix(X_num, label=y_full, weight=w_full)\n",
    "        mdl    = xgb.train(best_params, dtrain, num_boost_round=n_round)\n",
    "\n",
    "    elif key==\"hist_gbm\":\n",
    "        mdl = HistGradientBoostingClassifier(**best_params,\n",
    "                                             random_state=SEED)\n",
    "        mdl.fit(X_enc, y_full, sample_weight=w_full)\n",
    "\n",
    "    else:                       # random_forest\n",
    "        mdl = RandomForestClassifier(**best_params, n_jobs=-1,\n",
    "                                     random_state=SEED)\n",
    "        mdl.fit(X_enc, y_full, sample_weight=w_full)\n",
    "\n",
    "    joblib.dump(mdl, f\"{key}_FULL.pkl\")\n",
    "\n",
    "    #predict on test\n",
    "    def _proba(m, key):\n",
    "        if key in [\"hist_gbm\",\"random_forest\"]:\n",
    "            return m.predict_proba(X_enc.reindex(test_nodes.index))[:,1]\n",
    "        if key==\"xgboost\":\n",
    "            X_tmp = X_full.reindex(test_nodes.index).copy()\n",
    "            for c in CAT_COLS: X_tmp[c]=X_tmp[c].cat.codes\n",
    "            return m.predict(xgb.DMatrix(X_tmp))\n",
    "        if \"ranker\" in key:\n",
    "            return m.predict(X_full.reindex(test_nodes.index))\n",
    "        return m.predict_proba(X_full.reindex(test_nodes.index))[:,1]\n",
    "\n",
    "    p_vec = _proba(mdl, key)\n",
    "    col   = f\"p_{key}\"\n",
    "    test_nodes[col] = p_vec\n",
    "    prob_cols.append(col)\n",
    "    weights_w.append(results[key][0])\n",
    "\n",
    "    sub_i = (test_nodes\n",
    "             .loc[test_nodes.groupby([\"language\",\"sentence\"])[col].idxmax()]\n",
    "             .loc[:,[\"node\"]].rename(columns={\"node\":\"root\"}))\n",
    "    sub_i.to_csv(f\"submission_{key}.csv\", index=False)\n",
    "    print(f\"  saved  {key}_FULL.pkl  &  submission_{key}.csv\")\n",
    "\n",
    "#weighted ensemble of those K probability columns\n",
    "w_norm = np.array(weights_w)/np.sum(weights_w)\n",
    "test_nodes[\"p_ensemble\"] = (test_nodes[prob_cols] * w_norm).sum(1)\n",
    "\n",
    "sub_blend = (test_nodes\n",
    "             .loc[test_nodes.groupby([\"language\",\"sentence\"])[\"p_ensemble\"].idxmax()]\n",
    "             .loc[:,[\"node\"]].rename(columns={\"node\":\"root\"}))\n",
    "\n",
    "sub_blend.to_csv(\"submission_ensemble.csv\", index=False)\n",
    "print(\"\\n✔ blended submission saved → submission_ensemble.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f4d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
